{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c889240-d5a7-465d-9836-700eb7acd293",
   "metadata": {},
   "source": [
    "## **Data Format**\n",
    "* Data is taken from ERA5 website from 2013 to 2025. The data available is not in csv, json format it is in grib format\n",
    "* GRIB is a machine-optimized format designed to efficiently store huge, multi-dimensional weather datasets such as temperature, wind, pressure, humidity, rainfall, etc., over latitude–longitude grids, multiple heights, and many time steps.\n",
    "\n",
    "## **Data Preparation**\n",
    "* The dataset initially contained 7 columns, out of which number, step, surface, and valid_time were not relevant for the analysis and were therefore removed.\n",
    "* Temperature values were converted from Kelvin to Celsius.\n",
    "* To optimize memory usage, latitude and longitude were converted from float64 to float32, and temperature was converted from float32 to float16.\n",
    "\n",
    "## **Creating Unique IDS**\n",
    "* In the context of a time series model, an ID typically refers to an identifier that distinguishes different time series within the same dataset or model, especially when working with panel data or multiple time series simultaneously.\n",
    "* Latitude and longitude pairs can serve as potential IDs. To validate this, the number of unique (latitude, longitude) pairs was computed, resulting in 14,625 unique pairs.\n",
    "* Each latitude–longitude pair contains 8,760 data points, corresponding to hourly observations over one year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19fff4bc-bde3-41d7-aaf2-10c157c46bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import gc\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f96c5b-b40a-4b74-8bdb-c355816e72ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grib_dataset(file_path):\n",
    "    print(f\"Loading GRIB file: {file_path}\")\n",
    "    ds = xr.open_dataset(\n",
    "        file_path,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"indexpath\": \"\"}\n",
    "    )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e555343c-9188-495d-9c8b-892961319180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_year_processed(year, base_name=\"weather_data\"):\n",
    "    year_dir = Path(f\"{year}_{base_name}\")\n",
    "    if not year_dir.exists():\n",
    "        return False\n",
    "\n",
    "    parquet_files = list(year_dir.glob(\"weather_part_*.parquet\"))\n",
    "    return len(parquet_files) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec651e74-ff92-4790-9563-3e41812a8f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year_dataframe_lazy(ds, year):\n",
    "    print(f\"Extracting {year} safely...\")\n",
    "    ds_year = ds.sel(time=slice(f\"{year}-01-01\", f\"{year}-12-31\"))\n",
    "\n",
    "    df = ds_year.to_dataframe().reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0617a7d-05e0-4bd0-881c-b004343b4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weather_dataframe(df):\n",
    "    df = df.drop(columns=[\"number\", \"step\", \"surface\", \"valid_time\"], errors=\"ignore\")\n",
    "    df = df.rename(columns={\"t2m\": \"temperature\"})\n",
    "\n",
    "    df[\"temperature\"] = (df[\"temperature\"] - 273.15).astype(\"float16\")\n",
    "    df[\"latitude\"] = df[\"latitude\"].astype(\"float32\")\n",
    "    df[\"longitude\"] = df[\"longitude\"].astype(\"float32\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20cd06b5-ee05-40b8-8c3d-00740533dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_series_id(df, batch_size=1_000_000):\n",
    "    df[\"series_id\"] = None\n",
    "    n = len(df)\n",
    "\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(start + batch_size, n)\n",
    "\n",
    "        lat = df.iloc[start:end][\"latitude\"].round(2).astype(\"float32\")\n",
    "        lon = df.iloc[start:end][\"longitude\"].round(2).astype(\"float32\")\n",
    "\n",
    "        df.iloc[start:end, df.columns.get_loc(\"series_id\")] = (\n",
    "            \"<\" + lat.astype(str) + \", \" + lon.astype(str) + \">\"\n",
    "        ).values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e788e4f3-e885-4949-ae51-569224efbf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_series_chunks_to_parquet(df, year, n_files=5):\n",
    "    output_dir = Path(f\"{year}_weather_data\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    series_ids = df[\"series_id\"].unique()\n",
    "    series_id_chunks = np.array_split(series_ids, n_files)\n",
    "\n",
    "    for idx, chunk_ids in enumerate(series_id_chunks, 1):\n",
    "        chunk_df = df[df[\"series_id\"].isin(chunk_ids)]\n",
    "        chunk_df.to_parquet(output_dir / f\"weather_part_{idx}.parquet\", index=False)\n",
    "\n",
    "        del chunk_df\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"Saved parquet files for {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc149e2d-583f-4978-9223-cb05639933c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_year(file_path, year):\n",
    "    if is_year_processed(year):\n",
    "        print(f\"Skipping {year} — already processed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nProcessing year {year}\")\n",
    "\n",
    "    ds = load_grib_dataset(file_path)\n",
    "    df = extract_year_dataframe_lazy(ds, year)\n",
    "\n",
    "    del ds\n",
    "    gc.collect()\n",
    "\n",
    "    df = clean_weather_dataframe(df)\n",
    "    df = create_series_id(df)\n",
    "    df = df.drop(columns=[\"latitude\", \"longitude\"])\n",
    "\n",
    "    save_series_chunks_to_parquet(df, year)\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Finished processing {year}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd8c0931-7416-4cac-824e-1af5c271a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_years(file_path, years):\n",
    "    print(\"Starting multi-year processing...\\n\")\n",
    "\n",
    "    for year in years:\n",
    "        process_single_year(file_path, year)\n",
    "\n",
    "    print(\"\\nAll requested years checked/processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f0cee-7a51-49b8-8a06-3bef840ee8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_multiple_years(\n",
    "    file_path=\"data.grib\",\n",
    "    years=[2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6273e4b8-0af9-4964-b550-4abc49ba0449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_series_id_index_for_year(year, base_name=\"weather_data\"):\n",
    "    year_dir = Path(f\"{year}_{base_name}\")\n",
    "\n",
    "    parquet_files = sorted(year_dir.glob(\"weather_part_*.parquet\"))\n",
    "\n",
    "    if not parquet_files:\n",
    "        print(f\"No parquet files found for {year}\")\n",
    "        return\n",
    "\n",
    "    index_records = []\n",
    "\n",
    "    print(f\"\\nBuilding series_id index for {year}...\")\n",
    "\n",
    "    for file_path in parquet_files:\n",
    "        print(f\"Scanning {file_path.name}\")\n",
    "\n",
    "        df = pd.read_parquet(file_path, columns=[\"series_id\"])\n",
    "        unique_ids = df[\"series_id\"].unique()\n",
    "\n",
    "        index_records.extend(\n",
    "            {\"series_id\": sid, \"file_name\": file_path.name}\n",
    "            for sid in unique_ids\n",
    "        )\n",
    "\n",
    "        del df\n",
    "\n",
    "    index_df = pd.DataFrame(index_records)\n",
    "\n",
    "    index_path = year_dir / \"series_id_index.parquet\"\n",
    "    index_df.to_parquet(index_path, index=False)\n",
    "\n",
    "    print(f\"Index saved: {index_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16257090-6604-42e7-bdfd-343bd315d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_indices_for_years(years):\n",
    "    for year in years:\n",
    "        build_series_id_index_for_year(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "040c73cf-8646-44ec-8a3e-5271ce00a36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building series_id index for 2013...\n",
      "Scanning weather_part_1.parquet\n",
      "Scanning weather_part_2.parquet\n",
      "Scanning weather_part_3.parquet\n",
      "Scanning weather_part_4.parquet\n",
      "Scanning weather_part_5.parquet\n",
      "Index saved: 2013_weather_data\\series_id_index.parquet\n",
      "\n",
      "Building series_id index for 2014...\n",
      "Scanning weather_part_1.parquet\n",
      "Scanning weather_part_2.parquet\n",
      "Scanning weather_part_3.parquet\n",
      "Scanning weather_part_4.parquet\n",
      "Scanning weather_part_5.parquet\n",
      "Index saved: 2014_weather_data\\series_id_index.parquet\n",
      "\n",
      "Building series_id index for 2015...\n",
      "Scanning weather_part_1.parquet\n",
      "Scanning weather_part_2.parquet\n",
      "Scanning weather_part_3.parquet\n",
      "Scanning weather_part_4.parquet\n",
      "Scanning weather_part_5.parquet\n",
      "Index saved: 2015_weather_data\\series_id_index.parquet\n",
      "\n",
      "Building series_id index for 2016...\n",
      "Scanning weather_part_1.parquet\n",
      "Scanning weather_part_2.parquet\n",
      "Scanning weather_part_3.parquet\n",
      "Scanning weather_part_4.parquet\n",
      "Scanning weather_part_5.parquet\n",
      "Index saved: 2016_weather_data\\series_id_index.parquet\n",
      "\n",
      "Building series_id index for 2017...\n",
      "Scanning weather_part_1.parquet\n",
      "Scanning weather_part_2.parquet\n",
      "Scanning weather_part_3.parquet\n",
      "Scanning weather_part_4.parquet\n",
      "Scanning weather_part_5.parquet\n",
      "Index saved: 2017_weather_data\\series_id_index.parquet\n",
      "\n",
      "Building series_id index for 2018...\n",
      "Scanning weather_part_1.parquet\n",
      "Scanning weather_part_2.parquet\n",
      "Scanning weather_part_3.parquet\n",
      "Scanning weather_part_4.parquet\n",
      "Scanning weather_part_5.parquet\n",
      "Index saved: 2018_weather_data\\series_id_index.parquet\n",
      "\n",
      "Building series_id index for 2019...\n",
      "Scanning weather_part_1.parquet\n",
      "Scanning weather_part_2.parquet\n",
      "Scanning weather_part_3.parquet\n",
      "Scanning weather_part_4.parquet\n",
      "Scanning weather_part_5.parquet\n",
      "Index saved: 2019_weather_data\\series_id_index.parquet\n",
      "No parquet files found for 2020\n",
      "No parquet files found for 2021\n",
      "No parquet files found for 2022\n",
      "No parquet files found for 2023\n",
      "No parquet files found for 2024\n",
      "No parquet files found for 2025\n"
     ]
    }
   ],
   "source": [
    "build_indices_for_years([2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c5d1b5b-c431-4814-8af9-1d6c25f28cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_series_file(year, series_id, base_name=\"weather_data\"):\n",
    "    index_path = Path(f\"{year}_{base_name}\") / \"series_id_index.parquet\"\n",
    "\n",
    "    if not index_path.exists():\n",
    "        raise FileNotFoundError(f\"Index not found for year {year}. Build it first.\")\n",
    "\n",
    "    index_df = pd.read_parquet(index_path)\n",
    "\n",
    "    match = index_df[index_df[\"series_id\"] == series_id]\n",
    "\n",
    "    if match.empty:\n",
    "        print(f\"Series ID {series_id} not found for {year}\")\n",
    "        return None\n",
    "\n",
    "    file_name = match.iloc[0][\"file_name\"]\n",
    "    print(f\"Load this file: {file_name}\")\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59135ab4-bb05-44d3-99e0-c42e295f461d",
   "metadata": {},
   "source": [
    "## **Anomalies Data preparation**\n",
    "* Points that do not fall within the IQR range are considered anomalies.\n",
    "* The number of anomalies is very small; the percentage of non-anomalous points is significantly higher.\n",
    "* series_anomaly_summary is a nested dictionary that tracks, for each series_id, the number of anomalies in each respective year.\n",
    "* We restricted the analysis to series_ids that are present across all years, as some series appear in only one year and are missing in others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "599c199c-a3e2-401a-8b10-87da2d560330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_iqr_anomalies(group, temp_col, time_col):\n",
    "    q1 = group[temp_col].quantile(0.25)\n",
    "    q3 = group[temp_col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "\n",
    "    mask = (group[temp_col] < lower) | (group[temp_col] > upper)\n",
    "\n",
    "    return pd.Series({\n",
    "        \"anomaly_count\": int(mask.sum()),\n",
    "        \"anomaly_times\": group.loc[mask, time_col].tolist(),\n",
    "        \"anomaly_values\": group.loc[mask, temp_col].tolist()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad1deb9-4c3a-4da7-bc61-f3e3eee90387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weather_file_for_anomalies(\n",
    "    file_path,\n",
    "    temp_col=\"temperature\",\n",
    "    time_col=\"time\",\n",
    "    series_col=\"series_id\"\n",
    "):\n",
    "    print(f\"Processing: {file_path.name}\")\n",
    "\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "    df[\"year\"] = df[time_col].dt.year\n",
    "    df[\"month\"] = df[time_col].dt.month\n",
    "\n",
    "    result = (\n",
    "        df\n",
    "        .groupby(series_col, sort=False)\n",
    "        .apply(extract_iqr_anomalies, temp_col=temp_col, time_col=time_col)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6821ffcb-2858-497c-8100-983edb238b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year_for_anomalies(\n",
    "    year,\n",
    "    base_dir=\".\",\n",
    "    file_pattern=\"weather_part_*.parquet\",\n",
    "    temp_col=\"temperature\",\n",
    "    time_col=\"time\",\n",
    "    series_col=\"series_id\"\n",
    "):\n",
    "    data_dir = Path(base_dir) / f\"{year}_weather_data\"\n",
    "    anomaly_file = data_dir / f\"anomalies_{year}.parquet\"\n",
    "\n",
    "    # Skip if year folder missing\n",
    "    if not data_dir.exists():\n",
    "        print(f\"Skipping {year} — folder not found.\")\n",
    "        return None\n",
    "\n",
    "    if anomaly_file.exists():\n",
    "        print(f\"Skipping {year} — anomalies file already exists.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nProcessing anomalies for {year} in {data_dir}\")\n",
    "\n",
    "    all_files = sorted(data_dir.glob(file_pattern))\n",
    "    if not all_files:\n",
    "        print(f\"Skipping {year} — no parquet files found.\")\n",
    "        return None\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for file_path in all_files:\n",
    "        file_result = process_weather_file_for_anomalies(\n",
    "            file_path,\n",
    "            temp_col=temp_col,\n",
    "            time_col=time_col,\n",
    "            series_col=series_col\n",
    "        )\n",
    "        all_results.append(file_result)\n",
    "\n",
    "    final_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    # Save result\n",
    "    final_df.to_parquet(anomaly_file, index=False)\n",
    "\n",
    "    print(f\"Saved anomalies file: {anomaly_file}\")\n",
    "    print(f\"Finished {year}. Rows: {len(final_df)}\")\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce47e3b-8c05-4b33-9a73-d94c67be73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iqr_anomaly_pipeline_for_years(years, base_dir=\".\"):\n",
    "    combined_results = []\n",
    "\n",
    "    for year in years:\n",
    "        year_result = process_year_for_anomalies(year, base_dir=base_dir)\n",
    "\n",
    "        if year_result is not None:\n",
    "            combined_results.append(year_result)\n",
    "\n",
    "    if not combined_results:\n",
    "        print(\"No anomaly data generated.\")\n",
    "        return None\n",
    "\n",
    "    final_df = pd.concat(combined_results, ignore_index=True)\n",
    "\n",
    "    print(f\"\\nMulti-year anomaly detection complete. Total rows: {len(final_df)}\")\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab6b7041-10fa-4551-a3aa-2e9139a12351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing anomalies for 2013 in 2013_weather_data\n",
      "Processing: weather_part_1.parquet\n",
      "Processing: weather_part_2.parquet\n",
      "Processing: weather_part_3.parquet\n",
      "Processing: weather_part_4.parquet\n",
      "Processing: weather_part_5.parquet\n",
      "Saved anomalies file: 2013_weather_data\\anomalies_2013.parquet\n",
      "Finished 2013. Rows: 14625\n",
      "\n",
      "Processing anomalies for 2014 in 2014_weather_data\n",
      "Processing: weather_part_1.parquet\n",
      "Processing: weather_part_2.parquet\n",
      "Processing: weather_part_3.parquet\n",
      "Processing: weather_part_4.parquet\n",
      "Processing: weather_part_5.parquet\n",
      "Saved anomalies file: 2014_weather_data\\anomalies_2014.parquet\n",
      "Finished 2014. Rows: 14625\n",
      "\n",
      "Processing anomalies for 2015 in 2015_weather_data\n",
      "Processing: weather_part_1.parquet\n",
      "Processing: weather_part_2.parquet\n",
      "Processing: weather_part_3.parquet\n",
      "Processing: weather_part_4.parquet\n",
      "Processing: weather_part_5.parquet\n",
      "Saved anomalies file: 2015_weather_data\\anomalies_2015.parquet\n",
      "Finished 2015. Rows: 14625\n",
      "\n",
      "Processing anomalies for 2016 in 2016_weather_data\n",
      "Processing: weather_part_1.parquet\n",
      "Processing: weather_part_2.parquet\n",
      "Processing: weather_part_3.parquet\n",
      "Processing: weather_part_4.parquet\n",
      "Processing: weather_part_5.parquet\n",
      "Saved anomalies file: 2016_weather_data\\anomalies_2016.parquet\n",
      "Finished 2016. Rows: 14625\n",
      "\n",
      "Processing anomalies for 2017 in 2017_weather_data\n",
      "Processing: weather_part_1.parquet\n",
      "Processing: weather_part_2.parquet\n",
      "Processing: weather_part_3.parquet\n",
      "Processing: weather_part_4.parquet\n",
      "Processing: weather_part_5.parquet\n",
      "Saved anomalies file: 2017_weather_data\\anomalies_2017.parquet\n",
      "Finished 2017. Rows: 14625\n",
      "\n",
      "Processing anomalies for 2018 in 2018_weather_data\n",
      "Processing: weather_part_1.parquet\n",
      "Processing: weather_part_2.parquet\n",
      "Processing: weather_part_3.parquet\n",
      "Processing: weather_part_4.parquet\n",
      "Processing: weather_part_5.parquet\n",
      "Saved anomalies file: 2018_weather_data\\anomalies_2018.parquet\n",
      "Finished 2018. Rows: 14625\n",
      "\n",
      "Processing anomalies for 2019 in 2019_weather_data\n",
      "Processing: weather_part_1.parquet\n",
      "Processing: weather_part_2.parquet\n",
      "Processing: weather_part_3.parquet\n",
      "Processing: weather_part_4.parquet\n",
      "Processing: weather_part_5.parquet\n",
      "Saved anomalies file: 2019_weather_data\\anomalies_2019.parquet\n",
      "Finished 2019. Rows: 14625\n",
      "Skipping 2020 — folder not found.\n",
      "Skipping 2021 — folder not found.\n",
      "Skipping 2022 — folder not found.\n",
      "Skipping 2023 — folder not found.\n",
      "Skipping 2024 — folder not found.\n",
      "Skipping 2025 — folder not found.\n",
      "\n",
      "Multi-year anomaly detection complete. Total rows: 102375\n"
     ]
    }
   ],
   "source": [
    "all_anomalies = run_iqr_anomaly_pipeline_for_years(range(2013, 2026))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29777cc6-e940-4a5f-9e13-bc6f6fb9c7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>anomaly_count</th>\n",
       "      <th>anomaly_times</th>\n",
       "      <th>anomaly_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;37.0, 68.0&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;37.0, 68.25&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;37.0, 68.5&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;37.0, 68.75&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;37.0, 69.0&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       series_id  anomaly_count anomaly_times anomaly_values\n",
       "0   <37.0, 68.0>              0            []             []\n",
       "1  <37.0, 68.25>              0            []             []\n",
       "2   <37.0, 68.5>              0            []             []\n",
       "3  <37.0, 68.75>              0            []             []\n",
       "4   <37.0, 69.0>              0            []             []"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalies_2013 = pd.read_parquet('2013_weather_data/anomalies_2013.parquet')\n",
    "anomalies_2013.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "706385f9-6406-4e97-8609-75112477a3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>anomaly_count</th>\n",
       "      <th>anomaly_times</th>\n",
       "      <th>anomaly_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>&lt;37.0, 77.0&gt;</td>\n",
       "      <td>8</td>\n",
       "      <td>[2013-01-23T22:00:00.000000, 2013-02-06T23:00:...</td>\n",
       "      <td>[-30.859375, -30.71875, -31.046875, -31.015625...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>&lt;37.0, 85.0&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>[2013-01-06T00:00:00.000000]</td>\n",
       "      <td>[-37.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>&lt;37.0, 85.25&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>[2013-01-05T23:00:00.000000, 2013-01-06T00:00:...</td>\n",
       "      <td>[-39.15625, -38.625, -38.625, -39.125]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>&lt;37.0, 86.5&gt;</td>\n",
       "      <td>8</td>\n",
       "      <td>[2013-01-05T23:00:00.000000, 2013-01-06T00:00:...</td>\n",
       "      <td>[-41.75, -41.59375, -41.5625, -41.78125, -42.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>&lt;36.75, 77.0&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>[2013-02-07T03:00:00.000000, 2013-02-08T01:00:...</td>\n",
       "      <td>[-37.15625, -36.6875, -37.15625]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14604</th>\n",
       "      <td>&lt;6.0, 92.0&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>[2013-04-30T10:00:00.000000, 2013-04-30T11:00:...</td>\n",
       "      <td>[29.9375, 30.0, 29.984375, 29.984375, 29.90625]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14605</th>\n",
       "      <td>&lt;6.0, 92.25&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>[2013-04-30T10:00:00.000000, 2013-04-30T11:00:...</td>\n",
       "      <td>[29.921875, 29.984375, 30.0625, 30.015625]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14606</th>\n",
       "      <td>&lt;6.0, 92.5&gt;</td>\n",
       "      <td>6</td>\n",
       "      <td>[2013-04-29T14:00:00.000000, 2013-04-29T16:00:...</td>\n",
       "      <td>[29.90625, 29.984375, 29.90625, 30.015625, 30....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14607</th>\n",
       "      <td>&lt;6.0, 92.75&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>[2013-04-29T14:00:00.000000, 2013-04-29T15:00:...</td>\n",
       "      <td>[29.90625, 29.90625, 29.953125]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14608</th>\n",
       "      <td>&lt;6.0, 93.0&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>[2013-04-29T15:00:00.000000, 2013-04-29T16:00:...</td>\n",
       "      <td>[29.90625, 29.96875, 24.703125, 24.734375]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9144 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           series_id  anomaly_count  \\\n",
       "36      <37.0, 77.0>              8   \n",
       "68      <37.0, 85.0>              1   \n",
       "69     <37.0, 85.25>              4   \n",
       "74      <37.0, 86.5>              8   \n",
       "153    <36.75, 77.0>              3   \n",
       "...              ...            ...   \n",
       "14604    <6.0, 92.0>              5   \n",
       "14605   <6.0, 92.25>              4   \n",
       "14606    <6.0, 92.5>              6   \n",
       "14607   <6.0, 92.75>              3   \n",
       "14608    <6.0, 93.0>              4   \n",
       "\n",
       "                                           anomaly_times  \\\n",
       "36     [2013-01-23T22:00:00.000000, 2013-02-06T23:00:...   \n",
       "68                          [2013-01-06T00:00:00.000000]   \n",
       "69     [2013-01-05T23:00:00.000000, 2013-01-06T00:00:...   \n",
       "74     [2013-01-05T23:00:00.000000, 2013-01-06T00:00:...   \n",
       "153    [2013-02-07T03:00:00.000000, 2013-02-08T01:00:...   \n",
       "...                                                  ...   \n",
       "14604  [2013-04-30T10:00:00.000000, 2013-04-30T11:00:...   \n",
       "14605  [2013-04-30T10:00:00.000000, 2013-04-30T11:00:...   \n",
       "14606  [2013-04-29T14:00:00.000000, 2013-04-29T16:00:...   \n",
       "14607  [2013-04-29T14:00:00.000000, 2013-04-29T15:00:...   \n",
       "14608  [2013-04-29T15:00:00.000000, 2013-04-29T16:00:...   \n",
       "\n",
       "                                          anomaly_values  \n",
       "36     [-30.859375, -30.71875, -31.046875, -31.015625...  \n",
       "68                                               [-37.0]  \n",
       "69                [-39.15625, -38.625, -38.625, -39.125]  \n",
       "74     [-41.75, -41.59375, -41.5625, -41.78125, -42.0...  \n",
       "153                     [-37.15625, -36.6875, -37.15625]  \n",
       "...                                                  ...  \n",
       "14604    [29.9375, 30.0, 29.984375, 29.984375, 29.90625]  \n",
       "14605         [29.921875, 29.984375, 30.0625, 30.015625]  \n",
       "14606  [29.90625, 29.984375, 29.90625, 30.015625, 30....  \n",
       "14607                    [29.90625, 29.90625, 29.953125]  \n",
       "14608         [29.90625, 29.96875, 24.703125, 24.734375]  \n",
       "\n",
       "[9144 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalies_2013[anomalies_2013['anomaly_count'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "931f7404-4ee1-4184-8f5d-687bba1ce8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this code once when you get all the years anomalies data currently i am having data till 2020\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_year_anomaly_counts(year, base_dir=\".\"):\n",
    "    year_dir = Path(base_dir) / f\"{year}_weather_data\"\n",
    "    anomaly_file = year_dir / f\"anomalies_{year}.parquet\"\n",
    "\n",
    "    if not anomaly_file.exists():\n",
    "        print(f\"Skipping {year} — anomaly file not found.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Reading anomalies for {year}\")\n",
    "\n",
    "    # Load only needed columns\n",
    "    df = pd.read_parquet(anomaly_file, columns=[\"series_id\", \"anomaly_count\"])\n",
    "\n",
    "    df = df[df[\"anomaly_count\"] > 0]\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"No anomalies recorded for {year}\")\n",
    "        del df\n",
    "        gc.collect()\n",
    "        return None\n",
    "\n",
    "    # Aggregate per series_id\n",
    "    year_counts = df.groupby(\"series_id\")[\"anomaly_count\"].sum().to_dict()\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    return year_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6914b1e7-adbe-4d1b-9917-5b40a0c2804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_series_anomaly_dictionary(years, base_dir=\".\"):\n",
    "    series_anomaly_dict = defaultdict(dict)\n",
    "\n",
    "    for year in years:\n",
    "        year_counts = load_year_anomaly_counts(year, base_dir=base_dir)\n",
    "\n",
    "        if year_counts is None:\n",
    "            continue\n",
    "\n",
    "        for series_id, count in year_counts.items():\n",
    "            series_anomaly_dict[series_id][year] = int(count)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"Finished building anomaly dictionary\")\n",
    "    return dict(series_anomaly_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318b7301-effd-451d-bfbc-2fdf2bbaac6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading anomalies for 2013\n",
      "Reading anomalies for 2014\n",
      "Reading anomalies for 2015\n",
      "Reading anomalies for 2016\n",
      "Reading anomalies for 2017\n",
      "Reading anomalies for 2018\n",
      "Reading anomalies for 2019\n",
      "Skipping 2020 — anomaly file not found.\n",
      "Skipping 2021 — anomaly file not found.\n",
      "Skipping 2022 — anomaly file not found.\n",
      "Skipping 2023 — anomaly file not found.\n",
      "Skipping 2024 — anomaly file not found.\n",
      "Skipping 2025 — anomaly file not found.\n",
      "Finished building anomaly dictionary\n"
     ]
    }
   ],
   "source": [
    "series_anomaly_summary = build_series_anomaly_dictionary(range(2013, 2026))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd7bc00-1d77-412b-b886-f81ec4761ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11385"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(series_anomaly_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7c532d4-09f9-4ccb-887c-1ba9a9dd1d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series ids which are anomalies in all 7 years 5278\n"
     ]
    }
   ],
   "source": [
    "series_id_fluctuations = []\n",
    "for k,v in series_anomaly_summary.items():\n",
    "    if len(v.keys()) == 7:\n",
    "        series_id_fluctuations.append(k)\n",
    "print(f'Series ids which are anomalies in all 7 years {len(series_id_fluctuations)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9250642b-9058-4b55-9bcb-8df96e92d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def get_series_file_for_year(year, series_id, base_dir=\".\"):\n",
    "    index_path = Path(base_dir) / f\"{year}_weather_data\" / \"series_id_index.parquet\"\n",
    "\n",
    "    if not index_path.exists():\n",
    "        return None\n",
    "\n",
    "    idx_df = pd.read_parquet(index_path)\n",
    "    match = idx_df[idx_df[\"series_id\"] == series_id]\n",
    "\n",
    "    if match.empty:\n",
    "        return None\n",
    "\n",
    "    return match.iloc[0][\"file_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8892a53-a859-4df0-b454-58ccf1957ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "def save_filtered_series_dataset_chunked(series_id_list, years, base_dir=\".\", output_dir=\"filtered_anomaly_dataset\"):\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "\n",
    "    for year in years:\n",
    "        print(f\"\\nProcessing year {year}\")\n",
    "        year_dir = Path(base_dir) / f\"{year}_weather_data\"\n",
    "\n",
    "        if not year_dir.exists():\n",
    "            print(\"  Folder missing, skipping\")\n",
    "            continue\n",
    "\n",
    "        relevant_series = set(series_id_list)\n",
    "        parquet_files = sorted(year_dir.glob(\"weather_part_*.parquet\"))\n",
    "\n",
    "        for file_path in parquet_files:\n",
    "            print(f\"  Reading file {file_path.name}\")\n",
    "            df = pd.read_parquet(file_path, columns=[\"series_id\", \"temperature\"])\n",
    "\n",
    "            df = df[df[\"series_id\"].isin(relevant_series)]\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            # --- GROUPED IQR ---\n",
    "            def label_group(group):\n",
    "                q1 = group[\"temperature\"].quantile(0.25)\n",
    "                q3 = group[\"temperature\"].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower = q1 - 1.5 * iqr\n",
    "                upper = q3 + 1.5 * iqr\n",
    "                group[\"anomaly_label\"] = ((group[\"temperature\"] < lower) | (group[\"temperature\"] > upper)).astype(\"int8\")\n",
    "                return group[[\"series_id\", \"temperature\", \"anomaly_label\"]]\n",
    "\n",
    "            df = df.groupby(\"series_id\", group_keys=False).apply(label_group)\n",
    "\n",
    "            anomalies_count = df[\"anomaly_label\"].sum()\n",
    "            print(f\"    anomalies in chunk: {anomalies_count}\")\n",
    "\n",
    "            save_file = output_path / f\"{year}_{file_path.stem}_anomalies.parquet\"\n",
    "            df.to_parquet(save_file, index=False)\n",
    "\n",
    "            print(f\"    Saved chunk → {save_file}\")\n",
    "\n",
    "            del df\n",
    "            gc.collect()\n",
    "\n",
    "    print(\"\\nAll chunks saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbdd8e0a-d82a-49a9-9b2d-e0ea1cb6b688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing year 2013\n",
      "  Reading file weather_part_1.parquet\n",
      "    anomalies in chunk: 55\n",
      "    Saved chunk → filtered_anomaly_dataset\\2013_weather_part_1_anomalies.parquet\n",
      "  Reading file weather_part_2.parquet\n",
      "    anomalies in chunk: 25822\n",
      "    Saved chunk → filtered_anomaly_dataset\\2013_weather_part_2_anomalies.parquet\n",
      "  Reading file weather_part_3.parquet\n",
      "    anomalies in chunk: 788429\n",
      "    Saved chunk → filtered_anomaly_dataset\\2013_weather_part_3_anomalies.parquet\n",
      "  Reading file weather_part_4.parquet\n",
      "    anomalies in chunk: 241022\n",
      "    Saved chunk → filtered_anomaly_dataset\\2013_weather_part_4_anomalies.parquet\n",
      "  Reading file weather_part_5.parquet\n",
      "    anomalies in chunk: 106402\n",
      "    Saved chunk → filtered_anomaly_dataset\\2013_weather_part_5_anomalies.parquet\n",
      "\n",
      "Processing year 2014\n",
      "  Reading file weather_part_1.parquet\n",
      "    anomalies in chunk: 28\n",
      "    Saved chunk → filtered_anomaly_dataset\\2014_weather_part_1_anomalies.parquet\n",
      "  Reading file weather_part_2.parquet\n",
      "    anomalies in chunk: 10721\n",
      "    Saved chunk → filtered_anomaly_dataset\\2014_weather_part_2_anomalies.parquet\n",
      "  Reading file weather_part_3.parquet\n",
      "    anomalies in chunk: 399375\n",
      "    Saved chunk → filtered_anomaly_dataset\\2014_weather_part_3_anomalies.parquet\n",
      "  Reading file weather_part_4.parquet\n",
      "    anomalies in chunk: 149610\n",
      "    Saved chunk → filtered_anomaly_dataset\\2014_weather_part_4_anomalies.parquet\n",
      "  Reading file weather_part_5.parquet\n",
      "    anomalies in chunk: 64369\n",
      "    Saved chunk → filtered_anomaly_dataset\\2014_weather_part_5_anomalies.parquet\n",
      "\n",
      "Processing year 2015\n",
      "  Reading file weather_part_1.parquet\n",
      "    anomalies in chunk: 30\n",
      "    Saved chunk → filtered_anomaly_dataset\\2015_weather_part_1_anomalies.parquet\n",
      "  Reading file weather_part_2.parquet\n",
      "    anomalies in chunk: 21089\n",
      "    Saved chunk → filtered_anomaly_dataset\\2015_weather_part_2_anomalies.parquet\n",
      "  Reading file weather_part_3.parquet\n",
      "    anomalies in chunk: 438537\n",
      "    Saved chunk → filtered_anomaly_dataset\\2015_weather_part_3_anomalies.parquet\n",
      "  Reading file weather_part_4.parquet\n",
      "    anomalies in chunk: 143453\n",
      "    Saved chunk → filtered_anomaly_dataset\\2015_weather_part_4_anomalies.parquet\n",
      "  Reading file weather_part_5.parquet\n",
      "    anomalies in chunk: 32578\n",
      "    Saved chunk → filtered_anomaly_dataset\\2015_weather_part_5_anomalies.parquet\n",
      "\n",
      "Processing year 2016\n",
      "  Reading file weather_part_1.parquet\n",
      "    anomalies in chunk: 26\n",
      "    Saved chunk → filtered_anomaly_dataset\\2016_weather_part_1_anomalies.parquet\n",
      "  Reading file weather_part_2.parquet\n",
      "    anomalies in chunk: 25761\n",
      "    Saved chunk → filtered_anomaly_dataset\\2016_weather_part_2_anomalies.parquet\n",
      "  Reading file weather_part_3.parquet\n",
      "    anomalies in chunk: 746244\n",
      "    Saved chunk → filtered_anomaly_dataset\\2016_weather_part_3_anomalies.parquet\n",
      "  Reading file weather_part_4.parquet\n",
      "    anomalies in chunk: 353673\n",
      "    Saved chunk → filtered_anomaly_dataset\\2016_weather_part_4_anomalies.parquet\n",
      "  Reading file weather_part_5.parquet\n",
      "    anomalies in chunk: 192525\n",
      "    Saved chunk → filtered_anomaly_dataset\\2016_weather_part_5_anomalies.parquet\n",
      "\n",
      "Processing year 2017\n",
      "  Reading file weather_part_1.parquet\n",
      "    anomalies in chunk: 59\n",
      "    Saved chunk → filtered_anomaly_dataset\\2017_weather_part_1_anomalies.parquet\n",
      "  Reading file weather_part_2.parquet\n",
      "    anomalies in chunk: 12308\n",
      "    Saved chunk → filtered_anomaly_dataset\\2017_weather_part_2_anomalies.parquet\n",
      "  Reading file weather_part_3.parquet\n",
      "    anomalies in chunk: 531911\n",
      "    Saved chunk → filtered_anomaly_dataset\\2017_weather_part_3_anomalies.parquet\n",
      "  Reading file weather_part_4.parquet\n",
      "    anomalies in chunk: 218492\n",
      "    Saved chunk → filtered_anomaly_dataset\\2017_weather_part_4_anomalies.parquet\n",
      "  Reading file weather_part_5.parquet\n",
      "    anomalies in chunk: 58348\n",
      "    Saved chunk → filtered_anomaly_dataset\\2017_weather_part_5_anomalies.parquet\n",
      "\n",
      "Processing year 2018\n",
      "  Reading file weather_part_1.parquet\n",
      "    anomalies in chunk: 18\n",
      "    Saved chunk → filtered_anomaly_dataset\\2018_weather_part_1_anomalies.parquet\n",
      "  Reading file weather_part_2.parquet\n",
      "    anomalies in chunk: 15000\n",
      "    Saved chunk → filtered_anomaly_dataset\\2018_weather_part_2_anomalies.parquet\n",
      "  Reading file weather_part_3.parquet\n",
      "    anomalies in chunk: 473973\n",
      "    Saved chunk → filtered_anomaly_dataset\\2018_weather_part_3_anomalies.parquet\n",
      "  Reading file weather_part_4.parquet\n",
      "    anomalies in chunk: 124228\n",
      "    Saved chunk → filtered_anomaly_dataset\\2018_weather_part_4_anomalies.parquet\n",
      "  Reading file weather_part_5.parquet\n",
      "    anomalies in chunk: 73688\n",
      "    Saved chunk → filtered_anomaly_dataset\\2018_weather_part_5_anomalies.parquet\n",
      "\n",
      "Processing year 2019\n",
      "  Reading file weather_part_1.parquet\n",
      "    anomalies in chunk: 14\n",
      "    Saved chunk → filtered_anomaly_dataset\\2019_weather_part_1_anomalies.parquet\n",
      "  Reading file weather_part_2.parquet\n",
      "    anomalies in chunk: 12230\n",
      "    Saved chunk → filtered_anomaly_dataset\\2019_weather_part_2_anomalies.parquet\n",
      "  Reading file weather_part_3.parquet\n",
      "    anomalies in chunk: 634094\n",
      "    Saved chunk → filtered_anomaly_dataset\\2019_weather_part_3_anomalies.parquet\n",
      "  Reading file weather_part_4.parquet\n",
      "    anomalies in chunk: 205886\n",
      "    Saved chunk → filtered_anomaly_dataset\\2019_weather_part_4_anomalies.parquet\n",
      "  Reading file weather_part_5.parquet\n",
      "    anomalies in chunk: 40136\n",
      "    Saved chunk → filtered_anomaly_dataset\\2019_weather_part_5_anomalies.parquet\n",
      "\n",
      "All chunks saved successfully.\n"
     ]
    }
   ],
   "source": [
    "years = range(2013, 2020)\n",
    "\n",
    "save_filtered_series_dataset_chunked(\n",
    "    series_id_fluctuations,\n",
    "    years,\n",
    "    output_dir=\"filtered_anomaly_dataset\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
