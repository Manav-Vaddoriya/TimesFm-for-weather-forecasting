{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcJKmT8SYPsm"
      },
      "source": [
        "# **Fine-Tuning TimesFM for Anomaly Detection**\n",
        "\n",
        "## **Objective**\n",
        "This notebook implements a complete pipeline for fine-tuning a pretrained Time Series Foundation Model (TimesFM) for anomaly detection on weather data.\n",
        "\n",
        "## **Setup and Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwcaYGC1YPss",
        "outputId": "c5f84303-7ddd-403a-c135-f6ba49a075a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, precision_recall_curve, roc_curve, auc\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/anomaly_patches\"\n",
        "YEARS_TRAIN = [2013, 2014, 2015, 2016]\n",
        "YEARS_VAL = [2017, 2018]\n",
        "YEARS_TEST = [2019]\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 1e-4\n",
        "SAVE_DIR = \"/content/drive/MyDrive/models\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used google colab so to mount drive use this code\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X0RuXL1Ygg6",
        "outputId": "95fa4a0d-9f04-4b2e-e76e-d65832ce2c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5fwZ4DPYPsw"
      },
      "source": [
        "## **Model Architecture (Pretrained Backbone)**\n",
        "We reuse `TimesFMLiteGPT` architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acJA6Y9YYPsy"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"Root Mean Square Layer Normalization\"\"\"\n",
        "    def __init__(self, d_model, eps=1e-8):\n",
        "        super().__init__()\n",
        "        self.scale = nn.Parameter(torch.ones(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = torch.mean(x**2, dim=-1, keepdim=True)\n",
        "        x_normed = x * torch.rsqrt(norm + self.eps)\n",
        "        return self.scale * x_normed\n",
        "\n",
        "class TimesFMLiteGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.patch_embed = nn.Linear(config['patch_len'], config['d_model'])\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, config['context_len'], config['d_model']))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=config['d_model'],\n",
        "            nhead=config['n_heads'],\n",
        "            dim_feedforward=config['d_ff'],\n",
        "            dropout=config['dropout'],\n",
        "            activation=\"gelu\",\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.blocks = nn.TransformerEncoder(encoder_layer, num_layers=config['n_layers'])\n",
        "\n",
        "        self.norm_f = RMSNorm(config['d_model'])\n",
        "        self.head = nn.Linear(config['d_model'], config['patch_len'])\n",
        "\n",
        "    def forward(self, x, return_embeddings=False):\n",
        "        # x: [Batch, Seq_Len, Patch_Len]\n",
        "        B, T, P = x.shape\n",
        "\n",
        "        # Embedding\n",
        "        h = self.patch_embed(x)\n",
        "        h = h + self.pos_embed[:, :T, :]\n",
        "\n",
        "        # Causal Mask\n",
        "        mask = nn.Transformer.generate_square_subsequent_mask(T).to(x.device)\n",
        "\n",
        "        # Blocks\n",
        "        h = self.blocks(h, mask=mask, is_causal=True)\n",
        "        h = self.norm_f(h)\n",
        "\n",
        "        if return_embeddings:\n",
        "            return h\n",
        "\n",
        "        out = self.head(h)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiYlZFWsYPsz"
      },
      "source": [
        "## **Data Loading**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H5R4P4KYPsz"
      },
      "outputs": [],
      "source": [
        "class YearDataset(Dataset):\n",
        "    def __init__(self, year, base_path, context_len=32):\n",
        "        self.year = year\n",
        "        self.context_len = context_len\n",
        "\n",
        "        patch_path = os.path.join(base_path, str(year), \"temp_patches.npy\")\n",
        "        label_path = os.path.join(base_path, str(year), \"label_patches.npy\")\n",
        "\n",
        "        print(f\"Loading data for year {year}...\")\n",
        "        if not os.path.exists(patch_path) or not os.path.exists(label_path):\n",
        "            print(f\"WARNING: Data for year {year} not found at {patch_path} or {label_path}\")\n",
        "            self.data = np.zeros((0, 32), dtype=np.float32)\n",
        "            self.labels = np.zeros((0,), dtype=np.float32)\n",
        "            self.samples = []\n",
        "            return\n",
        "\n",
        "        # Use mmap_mode if memory is tight, but float32 usually fits fine\n",
        "        try:\n",
        "            self.data = np.load(patch_path).astype(np.float32)\n",
        "            self.labels = np.load(label_path).astype(np.float32)\n",
        "            print(f\"Loaded {os.path.basename(patch_path)}: {self.data.shape}, Labels: {self.labels.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading year {year}: {e}\")\n",
        "            self.data = np.zeros((0, 32), dtype=np.float32)\n",
        "            self.labels = np.zeros((0,), dtype=np.float32)\n",
        "\n",
        "        self.samples = []\n",
        "        num_patches = len(self.data)\n",
        "\n",
        "        if len(self.labels) != num_patches:\n",
        "            print(f\"Warning: Mismatch in data length {num_patches} and labels {len(self.labels)}\")\n",
        "\n",
        "        for i in range(num_patches - context_len + 1):\n",
        "            target_idx = i + context_len - 1\n",
        "            self.samples.append((i, target_idx))\n",
        "\n",
        "        print(f\"Year {year}: {len(self.samples)} sequences generated.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_idx, target_idx = self.samples[idx]\n",
        "        seq = self.data[start_idx : start_idx + self.context_len]\n",
        "        label = self.labels[target_idx]\n",
        "        label_scalar = np.max(label) if label.ndim > 0 else label\n",
        "\n",
        "        return torch.from_numpy(seq), torch.tensor(label_scalar, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_NyN8sGYPsz"
      },
      "source": [
        "## **Loss Functions for Imbalance**\n",
        "We implement Focal Loss to handle the rarity of anomalies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTrrRXXXYPs0"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.75, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        bce_loss = self.bce(inputs, targets)\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
        "        return focal_loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMQewjwrYPs0"
      },
      "source": [
        "## **Evaluation & Plotting Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C000d9P-YPs0"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history, save_name=\"training_plot.png\"):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history['train_loss'], label='Train Loss')\n",
        "    plt.plot(epochs, history['val_loss'], label='Val Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Metrics\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history['val_f1'], label='Val F1')\n",
        "    plt.plot(epochs, history['val_auroc'], label='Val AUROC')\n",
        "    plt.title('Validation Metrics')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_name)\n",
        "    plt.show()\n",
        "\n",
        "def save_model_checkpoint(model, name):\n",
        "    path = os.path.join(SAVE_DIR, f\"{name}.pth\")\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "def plot_roc_pr_curve(probs, targets, save_name=\"roc_pr_curve.png\"):\n",
        "    precision, recall, _ = precision_recall_curve(targets, probs)\n",
        "    fpr, tpr, _ = roc_curve(targets, probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # ROC Curve\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    # PR Curve\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label='PR curve')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_name)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5p_w6kAYPs1"
      },
      "source": [
        "## **Wrapper for Anomaly Classification**\n",
        "We wrap the base model to output a single scalar (anomaly score) for the sequence (or last patch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYvf6kZOYPs1"
      },
      "outputs": [],
      "source": [
        "class TimesFMAnomaly(nn.Module):\n",
        "    def __init__(self, pretrained_model, d_model):\n",
        "        super().__init__()\n",
        "        self.backbone = pretrained_model\n",
        "        # Classifier Head: Takes the embedding of the LAST patch to predict anomaly\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(d_model // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddings = self.backbone(x, return_embeddings=True)\n",
        "\n",
        "        # We pool the last time step for classification\n",
        "        last_embedding = embeddings[:, -1, :]\n",
        "\n",
        "        # Logits [BATCH, 1]\n",
        "        logits = self.classifier(last_embedding)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGDijGfwYPs1"
      },
      "source": [
        "# **Loading the data and splitting into train, test, validation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AQzxdC2YPs4",
        "outputId": "fa715fec-f076-45bc-9633-cdd2e2e8446d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructing Training Dataset...\n",
            "Loading data for year 2013...\n",
            "Loaded temp_patches.npy: (1446172, 32), Labels: (1446172, 32)\n",
            "Year 2013: 1446141 sequences generated.\n",
            "Loading data for year 2014...\n",
            "Loaded temp_patches.npy: (1446172, 32), Labels: (1446172, 32)\n",
            "Year 2014: 1446141 sequences generated.\n",
            "Loading data for year 2015...\n",
            "Loaded temp_patches.npy: (1446172, 32), Labels: (1446172, 32)\n",
            "Year 2015: 1446141 sequences generated.\n",
            "Loading data for year 2016...\n",
            "Loaded temp_patches.npy: (1451450, 32), Labels: (1451450, 32)\n",
            "Year 2016: 1451419 sequences generated.\n",
            "Constructing Validation Dataset...\n",
            "Loading data for year 2017...\n",
            "Loaded temp_patches.npy: (1446172, 32), Labels: (1446172, 32)\n",
            "Year 2017: 1446141 sequences generated.\n",
            "Loading data for year 2018...\n",
            "Loaded temp_patches.npy: (1446172, 32), Labels: (1446172, 32)\n",
            "Year 2018: 1446141 sequences generated.\n",
            "Constructing Test Dataset...\n",
            "Loading data for year 2019...\n",
            "Loaded temp_patches.npy: (1446172, 32), Labels: (1446172, 32)\n",
            "Year 2019: 1446141 sequences generated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "CONFIG = {\n",
        "    \"patch_len\": 32,\n",
        "    \"d_model\": 128,\n",
        "    \"n_layers\": 4,\n",
        "    \"n_heads\": 4,\n",
        "    \"d_ff\": 512,\n",
        "    \"dropout\": 0.1,\n",
        "    \"context_len\": 32,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"lr\": LEARNING_RATE,\n",
        "    \"epochs\": EPOCHS\n",
        "}\n",
        "\n",
        "# Create Datasets by Year\n",
        "def create_combined_dataset(years, base_path):\n",
        "    datasets = []\n",
        "    for y in years:\n",
        "        ds = YearDataset(y, base_path, context_len=CONFIG['context_len'])\n",
        "        if len(ds) > 0:\n",
        "            datasets.append(ds)\n",
        "    if not datasets:\n",
        "        return None\n",
        "    return ConcatDataset(datasets)\n",
        "\n",
        "print(\"Constructing Training Dataset...\")\n",
        "train_ds = create_combined_dataset(YEARS_TRAIN, DATA_DIR)\n",
        "print(\"Constructing Validation Dataset...\")\n",
        "val_ds = create_combined_dataset(YEARS_VAL, DATA_DIR)\n",
        "print(\"Constructing Test Dataset...\")\n",
        "test_ds = create_combined_dataset(YEARS_TEST, DATA_DIR)\n",
        "\n",
        "if train_ds:\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=CONFIG['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "       pin_memory=True\n",
        ")\n",
        "\n",
        "else:\n",
        "    print(\"CRITICAL ERROR: No training data found! Check paths.\")\n",
        "\n",
        "if val_ds:\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=CONFIG['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        ")\n",
        "\n",
        "if test_ds:\n",
        "    test_loader = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=CONFIG['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        ")\n",
        "\n",
        "# Instantiate Base Model\n",
        "base_model = TimesFMLiteGPT(CONFIG).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QulNZs3NYPs5"
      },
      "source": [
        "# **Transfer Learning**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "backbone = TimesFMLiteGPT(CONFIG).to(device)\n",
        "\n",
        "pretrained_path = \"/content/drive/MyDrive/weather_model_runs/best_model.pth\"  # YOUR PATH\n",
        "state_dict = torch.load(pretrained_path, map_location=device)\n",
        "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
        "\n",
        "backbone.load_state_dict(state_dict, strict=False)\n",
        "print(\"Pretrained backbone loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9KLEffnR7Hc",
        "outputId": "fc117c07-6dd9-4dd5-b598-423bd0c2b379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained backbone loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TimesFMAnomaly(backbone, CONFIG[\"d_model\"]).to(device)\n",
        "\n",
        "# Freeze backbone\n",
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Freeze RMSNorm explicitly\n",
        "for module in model.backbone.modules():\n",
        "    if isinstance(module, RMSNorm):\n",
        "        for p in module.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "# Sanity check\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable params: {trainable}/{total} ({100*trainable/total:.3f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b49fNujGSv8r",
        "outputId": "466cae44-f504-4564-fcce-cd4bbaebe8bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 8321/813985 (1.022%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = FocalLoss(alpha=0.75, gamma=2)\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.classifier.parameters(),\n",
        "    lr=CONFIG[\"lr\"]\n",
        ")"
      ],
      "metadata": {
        "id": "lYxdQjVFTRjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    average_precision_score\n",
        ")\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "def train_transfer_learning(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    epochs,\n",
        "    save_dir,\n",
        "):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_f1\": [],\n",
        "        \"val_precision\": [],\n",
        "        \"val_recall\": [],\n",
        "        \"val_auroc\": [],\n",
        "        \"val_aucpr\": []\n",
        "    }\n",
        "\n",
        "    best_val_auc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # TRAINING\n",
        "        model.backbone.eval()\n",
        "        model.classifier.train()\n",
        "\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device).unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "\n",
        "        # VALIDATION\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_probs, val_targets = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x = x.to(device)\n",
        "                y = y.to(device).unsqueeze(1)\n",
        "\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                probs = torch.sigmoid(logits)\n",
        "                val_probs.extend(probs.cpu().numpy().ravel())\n",
        "                val_targets.extend(y.cpu().numpy().ravel())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        val_probs = np.array(val_probs)\n",
        "        val_targets = np.array(val_targets)\n",
        "\n",
        "        # METRICS\n",
        "        val_auc = roc_auc_score(val_targets, val_probs)\n",
        "        val_aucpr = average_precision_score(val_targets, val_probs)\n",
        "\n",
        "        # Threshold 0.5 is often suboptimal for anomalies\n",
        "        val_preds = (val_probs >= 0.2).astype(int)\n",
        "\n",
        "        val_precision = precision_score(val_targets, val_preds, zero_division=0)\n",
        "        val_recall = recall_score(val_targets, val_preds, zero_division=0)\n",
        "        val_f1 = f1_score(val_targets, val_preds, zero_division=0)\n",
        "\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_auroc\"].append(val_auc)\n",
        "        history[\"val_aucpr\"].append(val_aucpr)\n",
        "        history[\"val_precision\"].append(val_precision)\n",
        "        history[\"val_recall\"].append(val_recall)\n",
        "        history[\"val_f1\"].append(val_f1)\n",
        "\n",
        "        # LOGGING\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{epochs}] | \"\n",
        "            f\"Train Loss: {train_loss:.4f} | \"\n",
        "            f\"Val Loss: {val_loss:.4f} | \"\n",
        "            f\"AUC: {val_auc:.4f} | \"\n",
        "            f\"AUC-PR: {val_aucpr:.4f} | \"\n",
        "            f\"Prec: {val_precision:.4f} | \"\n",
        "            f\"Rec: {val_recall:.4f} | \"\n",
        "            f\"F1: {val_f1:.4f}\"\n",
        "        )\n",
        "\n",
        "        # SAVE BEST MODEL (AUROC)\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(save_dir, \"best_transfer_model.pth\")\n",
        "            )\n",
        "\n",
        "    print(f\"\\nBest Validation AUROC: {best_val_auc:.4f}\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "SOpDvehQTS0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = \"/content/drive/MyDrive/models\"\n",
        "\n",
        "history = train_transfer_learning(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    epochs=CONFIG[\"epochs\"],\n",
        "    save_dir=SAVE_DIR\n",
        ")"
      ],
      "metadata": {
        "id": "1d6GOlj1T30t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f77b378-c270-46d1-bf87-0fccb57359b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3] | Train Loss: 0.0361 | Val Loss: 0.0362 | AUC: 0.9331 | AUC-PR: 0.6276 | Prec: 0.2464 | Rec: 0.9728 | F1: 0.3932\n",
            "Epoch [2/3] | Train Loss: 0.0344 | Val Loss: 0.0364 | AUC: 0.9324 | AUC-PR: 0.6250 | Prec: 0.2663 | Rec: 0.9636 | F1: 0.4172\n",
            "Epoch [3/3] | Train Loss: 0.0339 | Val Loss: 0.0359 | AUC: 0.9346 | AUC-PR: 0.6306 | Prec: 0.2427 | Rec: 0.9773 | F1: 0.3888\n",
            "\n",
            "Best Validation AUROC: 0.9346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_preds, test_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(x)\n",
        "        test_preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
        "        test_targets.extend(y.cpu().numpy())\n",
        "\n",
        "test_auc = roc_auc_score(test_targets, test_preds)\n",
        "print(f\"Test AUC: {test_auc:.4f}\")"
      ],
      "metadata": {
        "id": "lhDWeVoxT8in",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792f92e1-83b9-45a5-dfd4-f7a4e24d307d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test AUC: 0.9392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNTuvJHGYPs6"
      },
      "source": [
        "### Experiment 3: LoRA (Low-Rank Adaptation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un3vpB9VYPs6"
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, linear_layer, rank=4, alpha=16):\n",
        "        super().__init__()\n",
        "        self.linear = linear_layer\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "\n",
        "        in_dim = linear_layer.in_features\n",
        "        out_dim = linear_layer.out_features\n",
        "\n",
        "        self.lora_A = nn.Parameter(torch.zeros(in_dim, rank))\n",
        "        self.lora_B = nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        self.scaling = alpha / rank\n",
        "\n",
        "        # Initialize\n",
        "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_B)\n",
        "\n",
        "        # Freeze original\n",
        "        self.linear.weight.requires_grad = False\n",
        "        if self.linear.bias is not None:\n",
        "            self.linear.bias.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        regular_out = self.linear(x)\n",
        "        lora_out = (x @ self.lora_A @ self.lora_B) * self.scaling\n",
        "        return regular_out + lora_out\n",
        "\n",
        "def apply_lora(model, rank=4):\n",
        "    # Apply LoRA to patch_embed and output head (simplified for demo)\n",
        "    model.patch_embed = LoRALayer(model.patch_embed, rank=rank)\n",
        "    return model\n",
        "\n",
        "print(\"Initializing LoRA...\")\n",
        "lora_base = copy.deepcopy(base_model)\n",
        "lora_base = apply_lora(lora_base)\n",
        "lora_model = TimesFMAnomaly(lora_base, CONFIG['d_model'])\n",
        "\n",
        "# Ensure only LoRA params and Head are trainable\n",
        "param_count = 0\n",
        "trainable = 0\n",
        "for n, p in lora_model.named_parameters():\n",
        "    param_count += p.numel()\n",
        "    if 'lora' in n or 'classifier' in n:\n",
        "        p.requires_grad = True\n",
        "        trainable += p.numel()\n",
        "    else:\n",
        "        p.requires_grad = False\n",
        "\n",
        "print(f\"LoRA Trainable Params: {trainable} / {param_count} ({trainable/param_count:.2%})\")\n",
        "lora_trained = train_experiment(lora_model, \"LoRA Fine-Tuning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uISUQTmWYPs6"
      },
      "source": [
        "### Experiment 4: Model Distillation\n",
        "We use the Full-Fine-Tuned model as Teacher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYsPzFjaYPs6"
      },
      "outputs": [],
      "source": [
        "print(\"Initializing Distillation...\")\n",
        "\n",
        "# Teacher:  Full Fine-Tuned Model (fft_trained)\n",
        "teacher_model = fft_trained\n",
        "teacher_model.eval()\n",
        "\n",
        "# Student: Smaller config\n",
        "STUDENT_CONFIG = CONFIG.copy()\n",
        "STUDENT_CONFIG['n_layers'] = 2 # Half layers\n",
        "STUDENT_CONFIG['d_model'] = 64 # Half width\n",
        "\n",
        "student_base = TimesFMLiteGPT(STUDENT_CONFIG).to(device)\n",
        "student_model = TimesFMAnomaly(student_base, STUDENT_CONFIG['d_model']).to(device)\n",
        "\n",
        "def distillation_loss(student_logits, teacher_logits, targets, T=2.0, alpha=0.5):\n",
        "    # Soft Target Loss (KLDiv)\n",
        "    teacher_probs = torch.sigmoid(teacher_logits / T)\n",
        "    student_probs = torch.sigmoid(student_logits / T)\n",
        "    # Standard binary distillation\n",
        "    soft_loss = nn.MSELoss()(student_probs, teacher_probs)\n",
        "\n",
        "    # Hard Target Loss (Focal)\n",
        "    hard_loss = FocalLoss()(student_logits, targets)\n",
        "\n",
        "    return alpha * soft_loss + (1 - alpha) * hard_loss\n",
        "\n",
        "optimizer = torch.optim.AdamW(student_model.parameters(), lr=CONFIG['lr'])\n",
        "\n",
        "print(\"--- Starting Distillation ---\")\n",
        "history = {'train_loss': [], 'val_loss': [], 'val_f1': [], 'val_auroc': []}\n",
        "\n",
        "for epoch in range(CONFIG['epochs']):\n",
        "    student_model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y = y.unsqueeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = teacher_model(x)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        student_logits = student_model(x)\n",
        "\n",
        "        loss = distillation_loss(student_logits, teacher_logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    metrics, _ = evaluate_anomaly(student_model, val_loader, device)\n",
        "    print(f\"Epoch {epoch+1}: Student F1={metrics['F1']:.4f}, AUROC={metrics['AUROC']:.4f}\")\n",
        "\n",
        "    history['train_loss'].append(total_loss / len(train_loader))\n",
        "    history['val_loss'].append(metrics['Loss'])\n",
        "    history['val_f1'].append(metrics['F1'])\n",
        "    history['val_auroc'].append(metrics['AUROC'])\n",
        "\n",
        "save_model_checkpoint(student_model, \"distilled_student_model\")\n",
        "plot_training_history(history, save_name=\"distillation_curves.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0Npv9mVYPs7"
      },
      "source": [
        "### Experiment 5: Continuous Pre-Training\n",
        "Continue the self-supervised task (Next Patch Prediction) on the new domain data, then fine-tune head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOqsLvsnYPs7"
      },
      "outputs": [],
      "source": [
        "print(\"Initializing Continuous Pre-Training...\")\n",
        "# 1. Unsupervised Phase (Reconstruction / Next Token)\n",
        "cpt_base = copy.deepcopy(base_model)\n",
        "optimizer = torch.optim.AdamW(cpt_base.parameters(), lr=CONFIG['lr'])\n",
        "criterion_mse = nn.MSELoss()\n",
        "\n",
        "print(\"Phase 1: Self-Supervised Update\")\n",
        "for epoch in range(2): # Short phase e.g. 2 epochs\n",
        "    cpt_base.train()\n",
        "    for x, _ in train_loader:\n",
        "        x = x.to(device)\n",
        "        # Simulate Next-Patch Prediction task:\n",
        "        # Input: 0..N-1, Target: 1..N\n",
        "        # x shape: [B, Context, Patch]\n",
        "        inp = x[:, :-1, :]\n",
        "        target = x[:, 1:, :]\n",
        "\n",
        "        if inp.size(1) == 0: continue\n",
        "\n",
        "        pred = cpt_base(inp)\n",
        "        loss = criterion_mse(pred, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"CPT Epoch {epoch+1} MSE: {loss.item():.4f}\")\n",
        "\n",
        "# 2. Fine-Tune Head for Anomaly\n",
        "print(\"Phase 2: Anomaly Head Tuning\")\n",
        "cpt_model = TimesFMAnomaly(cpt_base, CONFIG['d_model'])\n",
        "cpt_trained = train_experiment(cpt_model, \"Continuous Pre-Training + Fine-Tuning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NpUDd6jYPs7"
      },
      "source": [
        "### Final Evaluation on Test Set\n",
        "We evaluate the best performing model (example: LoRA or Full FT) on the held-out Test Set (2019)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOlPEd69YPs7"
      },
      "outputs": [],
      "source": [
        "print(\"--- TEST SET EVALUATION (LoRA Model) ---\")\n",
        "# Using Lora Model as example\n",
        "metrics, (probs, targets) = evaluate_anomaly(lora_model, test_loader, device)\n",
        "print(f\"Test Set Results: F1={metrics['F1']:.4f}, AUROC={metrics['AUROC']:.4f}, AUPRC={metrics['AUPRC']:.4f}\")\n",
        "\n",
        "# Plot ROC and PR Curves for Test Set\n",
        "plot_roc_pr_curve(probs, targets, save_name=\"final_test_roc_pr.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}